{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901c8ef3",
      "metadata": {
        "id": "901c8ef3"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1662bb7c",
      "metadata": {
        "id": "1662bb7c"
      },
      "source": [
        "# Produces masks from prompts using an ONNX model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fcc21a0",
      "metadata": {
        "id": "7fcc21a0"
      },
      "source": [
        "SAM's prompt encoder and mask decoder are very lightweight, which allows for efficient computation of a mask given user input. This notebook shows an example of how to export and use this lightweight component of the model in ONNX format, allowing it to run on a variety of platforms that support an ONNX runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86daff77",
      "metadata": {
        "id": "86daff77",
        "outputId": "4ae4a6ff-8052-41e5-cf31-23c849f53535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\">\n",
              "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
              "</a>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\n",
        "\"\"\"\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ae4e00",
      "metadata": {
        "id": "55ae4e00"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109a5cc2",
      "metadata": {
        "id": "109a5cc2"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. The latest stable versions of PyTorch and ONNX are recommended for this notebook. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "39b99fc4",
      "metadata": {
        "id": "39b99fc4"
      },
      "outputs": [],
      "source": [
        "using_colab = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/facebookresearch/segment-anything.git"
      ],
      "metadata": {
        "id": "CA1OIjHYQtnw",
        "outputId": "da58b528-a7a4-4ce4-fe3f-57a3e313e95d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CA1OIjHYQtnw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-gy2vmbai\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-gy2vmbai\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36587 sha256=8211100e6284a75675da6a91f5d7346907155a6fe5c597b89e89062d2094cd65\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z03c1tts/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "296a69be",
      "metadata": {
        "id": "296a69be"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib onnx onnxruntime\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python pycocotools matplotlib onnxruntime onnx"
      ],
      "metadata": {
        "id": "wUKabs35RJ04",
        "outputId": "7d169c85-212b-4df9-a4e2-23336df0a56f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wUKabs35RJ04",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.15.0 onnxruntime-1.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc4a58be",
      "metadata": {
        "id": "dc4a58be"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42396e8d",
      "metadata": {
        "id": "42396e8d"
      },
      "source": [
        "Note that this notebook requires both the `onnx` and `onnxruntime` optional dependencies, in addition to `opencv-python` and `matplotlib` for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2c712610",
      "metadata": {
        "id": "2c712610"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from segment_anything.utils.onnx import SamOnnxModel\n",
        "\n",
        "import onnxruntime\n",
        "from onnxruntime.quantization import QuantType\n",
        "from onnxruntime.quantization.quantize import quantize_dynamic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f29441b9",
      "metadata": {
        "id": "f29441b9"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax):\n",
        "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0f6b2b",
      "metadata": {
        "id": "bd0f6b2b"
      },
      "source": [
        "## Export an ONNX model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1540f719",
      "metadata": {
        "id": "1540f719"
      },
      "source": [
        "Set the path below to a SAM model checkpoint, then load the model. This will be needed to both export the model and to calculate embeddings for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "76fc53f4",
      "metadata": {
        "id": "76fc53f4"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "yXrXCzSXWZMN",
        "outputId": "bc9c1076-39a4-423a-c682-f1e8b6cd5cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "id": "yXrXCzSXWZMN",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3ecd6bb2-82cf-4e5f-a084-ecca67e643f8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3ecd6bb2-82cf-4e5f-a084-ecca67e643f8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11bfc8aa",
      "metadata": {
        "id": "11bfc8aa"
      },
      "outputs": [],
      "source": [
        "sam = sam_model_registry[model_type](checkpoint=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "450c089c",
      "metadata": {
        "id": "450c089c"
      },
      "source": [
        "The script `segment-anything/scripts/export_onnx_model.py` can be used to export the necessary portion of SAM. Alternatively, run the following code to export an ONNX model. If you have already exported a model, set the path below and skip to the next section. Assure that the exported ONNX model aligns with the checkpoint and model type set above. This notebook expects the model was exported with the parameter `return_single_mask=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "38a8add8",
      "metadata": {
        "id": "38a8add8"
      },
      "outputs": [],
      "source": [
        "onnx_model_path = None  # Set to use an already exported model, then skip to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da638ba",
      "metadata": {
        "scrolled": false,
        "id": "7da638ba"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "onnx_model_path = \"sam_onnx_example.onnx\"\n",
        "\n",
        "onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
        "\n",
        "dynamic_axes = {\n",
        "    \"point_coords\": {1: \"num_points\"},\n",
        "    \"point_labels\": {1: \"num_points\"},\n",
        "}\n",
        "\n",
        "embed_dim = sam.prompt_encoder.embed_dim\n",
        "embed_size = sam.prompt_encoder.image_embedding_size\n",
        "mask_input_size = [4 * x for x in embed_size]\n",
        "dummy_inputs = {\n",
        "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
        "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
        "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
        "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
        "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
        "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
        "}\n",
        "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    with open(onnx_model_path, \"wb\") as f:\n",
        "        torch.onnx.export(\n",
        "            onnx_model,\n",
        "            tuple(dummy_inputs.values()),\n",
        "            f,\n",
        "            export_params=True,\n",
        "            verbose=False,\n",
        "            opset_version=17,\n",
        "            do_constant_folding=True,\n",
        "            input_names=list(dummy_inputs.keys()),\n",
        "            output_names=output_names,\n",
        "            dynamic_axes=dynamic_axes,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c450cf1a",
      "metadata": {
        "id": "c450cf1a"
      },
      "source": [
        "If desired, the model can additionally be quantized and optimized. We find this improves web runtime significantly for negligible change in qualitative performance. Run the next cell to quantize the model, or skip to the next section otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235d39fe",
      "metadata": {
        "id": "235d39fe"
      },
      "outputs": [],
      "source": [
        "onnx_model_quantized_path = \"sam_onnx_quantized_example.onnx\"\n",
        "quantize_dynamic(\n",
        "    model_input=onnx_model_path,\n",
        "    model_output=onnx_model_quantized_path,\n",
        "    optimize_model=True,\n",
        "    per_channel=False,\n",
        "    reduce_range=False,\n",
        "    weight_type=QuantType.QUInt8,\n",
        ")\n",
        "onnx_model_path = onnx_model_quantized_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927a928b",
      "metadata": {
        "id": "927a928b"
      },
      "source": [
        "## Example Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be6eb55",
      "metadata": {
        "id": "6be6eb55"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('images/truck.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e9a27a",
      "metadata": {
        "id": "b7e9a27a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.axis('on')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027b177b",
      "metadata": {
        "id": "027b177b"
      },
      "source": [
        "## Using an ONNX model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "778d4593",
      "metadata": {
        "id": "778d4593"
      },
      "source": [
        "Here as an example, we use `onnxruntime` in python on CPU to execute the ONNX model. However, any platform that supports an ONNX runtime could be used in principle. Launch the runtime session below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9689b1bf",
      "metadata": {
        "id": "9689b1bf"
      },
      "outputs": [],
      "source": [
        "ort_session = onnxruntime.InferenceSession(onnx_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7708ead6",
      "metadata": {
        "id": "7708ead6"
      },
      "source": [
        "To use the ONNX model, the image must first be pre-processed using the SAM image encoder. This is a heavier weight process best performed on GPU. SamPredictor can be used as normal, then `.get_image_embedding()` will retreive the intermediate features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e067b4",
      "metadata": {
        "id": "26e067b4"
      },
      "outputs": [],
      "source": [
        "sam.to(device='cuda')\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ad3f0d6",
      "metadata": {
        "id": "7ad3f0d6"
      },
      "outputs": [],
      "source": [
        "predictor.set_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6f0f07",
      "metadata": {
        "id": "8a6f0f07"
      },
      "outputs": [],
      "source": [
        "image_embedding = predictor.get_image_embedding().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e112f33",
      "metadata": {
        "id": "5e112f33"
      },
      "outputs": [],
      "source": [
        "image_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6337b654",
      "metadata": {
        "id": "6337b654"
      },
      "source": [
        "The ONNX model has a different input signature than `SamPredictor.predict`. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are `np.float32`.\n",
        "* `image_embeddings`: The image embedding from `predictor.get_image_embedding()`. Has a batch index of length 1.\n",
        "* `point_coords`: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. *Coordinates must already be transformed to long-side 1024.* Has a batch index of length 1.\n",
        "* `point_labels`: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. *If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.*\n",
        "* `mask_input`: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.\n",
        "* `has_mask_input`: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.\n",
        "* `orig_im_size`: The size of the input image in (H,W) format, before any transformation.\n",
        "\n",
        "Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at `sam.mask_threshold` (equal to 0.0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5a9f55",
      "metadata": {
        "id": "bf5a9f55"
      },
      "source": [
        "### Example point input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0deef0",
      "metadata": {
        "id": "1c0deef0"
      },
      "outputs": [],
      "source": [
        "input_point = np.array([[500, 375]])\n",
        "input_label = np.array([1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7256394c",
      "metadata": {
        "id": "7256394c"
      },
      "source": [
        "Add a batch index, concatenate a padding point, and transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f69903e",
      "metadata": {
        "id": "4f69903e"
      },
      "outputs": [],
      "source": [
        "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
        "onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n",
        "\n",
        "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b188dc53",
      "metadata": {
        "id": "b188dc53"
      },
      "source": [
        "Create an empty mask input and an indicator for no mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb52bcf",
      "metadata": {
        "id": "5cb52bcf"
      },
      "outputs": [],
      "source": [
        "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
        "onnx_has_mask_input = np.zeros(1, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a99c2cc5",
      "metadata": {
        "id": "a99c2cc5"
      },
      "source": [
        "Package the inputs to run in the onnx model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d7ea11",
      "metadata": {
        "id": "b1d7ea11"
      },
      "outputs": [],
      "source": [
        "ort_inputs = {\n",
        "    \"image_embeddings\": image_embedding,\n",
        "    \"point_coords\": onnx_coord,\n",
        "    \"point_labels\": onnx_label,\n",
        "    \"mask_input\": onnx_mask_input,\n",
        "    \"has_mask_input\": onnx_has_mask_input,\n",
        "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6409c9",
      "metadata": {
        "id": "4b6409c9"
      },
      "source": [
        "Predict a mask and threshold it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4cc082",
      "metadata": {
        "scrolled": false,
        "id": "dc4cc082"
      },
      "outputs": [],
      "source": [
        "masks, _, low_res_logits = ort_session.run(None, ort_inputs)\n",
        "masks = masks > predictor.model.mask_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d778a8fb",
      "metadata": {
        "id": "d778a8fb"
      },
      "outputs": [],
      "source": [
        "masks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "badb1175",
      "metadata": {
        "id": "badb1175"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "show_mask(masks, plt.gca())\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1d4d15",
      "metadata": {
        "id": "1f1d4d15"
      },
      "source": [
        "### Example mask input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b319da82",
      "metadata": {
        "id": "b319da82"
      },
      "outputs": [],
      "source": [
        "input_point = np.array([[500, 375], [1125, 625]])\n",
        "input_label = np.array([1, 1])\n",
        "\n",
        "# Use the mask output from the previous run. It is already in the correct form for input to the ONNX model.\n",
        "onnx_mask_input = low_res_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1823b37",
      "metadata": {
        "id": "b1823b37"
      },
      "source": [
        "Transform the points as in the previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8885130f",
      "metadata": {
        "id": "8885130f"
      },
      "outputs": [],
      "source": [
        "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
        "onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n",
        "\n",
        "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28e47b69",
      "metadata": {
        "id": "28e47b69"
      },
      "source": [
        "The `has_mask_input` indicator is now 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab4483a",
      "metadata": {
        "id": "3ab4483a"
      },
      "outputs": [],
      "source": [
        "onnx_has_mask_input = np.ones(1, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3781955",
      "metadata": {
        "id": "d3781955"
      },
      "source": [
        "Package inputs, then predict and threshold the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ec096",
      "metadata": {
        "id": "0c1ec096"
      },
      "outputs": [],
      "source": [
        "ort_inputs = {\n",
        "    \"image_embeddings\": image_embedding,\n",
        "    \"point_coords\": onnx_coord,\n",
        "    \"point_labels\": onnx_label,\n",
        "    \"mask_input\": onnx_mask_input,\n",
        "    \"has_mask_input\": onnx_has_mask_input,\n",
        "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
        "}\n",
        "\n",
        "masks, _, _ = ort_session.run(None, ort_inputs)\n",
        "masks = masks > predictor.model.mask_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e36554b",
      "metadata": {
        "id": "1e36554b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "show_mask(masks, plt.gca())\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef211d0",
      "metadata": {
        "id": "2ef211d0"
      },
      "source": [
        "### Example box and point input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e58d2e",
      "metadata": {
        "id": "51e58d2e"
      },
      "outputs": [],
      "source": [
        "input_box = np.array([425, 600, 700, 875])\n",
        "input_point = np.array([[575, 750]])\n",
        "input_label = np.array([0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e119dcb",
      "metadata": {
        "id": "6e119dcb"
      },
      "source": [
        "Add a batch index, concatenate a box and point inputs, add the appropriate labels for the box corners, and transform. There is no padding point since the input includes a box input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfbe4911",
      "metadata": {
        "id": "bfbe4911"
      },
      "outputs": [],
      "source": [
        "onnx_box_coords = input_box.reshape(2, 2)\n",
        "onnx_box_labels = np.array([2,3])\n",
        "\n",
        "onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]\n",
        "onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)\n",
        "\n",
        "onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65edabd2",
      "metadata": {
        "id": "65edabd2"
      },
      "source": [
        "Package inputs, then predict and threshold the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2abfba56",
      "metadata": {
        "id": "2abfba56"
      },
      "outputs": [],
      "source": [
        "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
        "onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
        "\n",
        "ort_inputs = {\n",
        "    \"image_embeddings\": image_embedding,\n",
        "    \"point_coords\": onnx_coord,\n",
        "    \"point_labels\": onnx_label,\n",
        "    \"mask_input\": onnx_mask_input,\n",
        "    \"has_mask_input\": onnx_has_mask_input,\n",
        "    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n",
        "}\n",
        "\n",
        "masks, _, _ = ort_session.run(None, ort_inputs)\n",
        "masks = masks > predictor.model.mask_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8301bf33",
      "metadata": {
        "id": "8301bf33"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "show_mask(masks[0], plt.gca())\n",
        "show_box(input_box, plt.gca())\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}